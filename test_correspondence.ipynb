{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU/MPS available, falling back to CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from mesh_video_generator import MeshVideoGenerator\\n\\nhw = 512\\nnum_views = 10\\n\\ngenerator = MeshVideoGenerator(\\n    output_dir=\"outputs\",\\n    hw=hw,\\n    num_views=num_views,\\n    use_normal_map=True,\\n    device=device\\n) \\ngenerator.process_folder(\"camel_tex\", display_frames=False)\\n#generator.process_single_mesh(\"meshes\", index=5) '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from mesh_video_generator import MeshVideoGenerator\n",
    "\n",
    "hw = 512\n",
    "num_views = 10\n",
    "\n",
    "generator = MeshVideoGenerator(\n",
    "    output_dir=\"outputs\",\n",
    "    hw=hw,\n",
    "    num_views=num_views,\n",
    "    use_normal_map=True,\n",
    "    device=device\n",
    ") \n",
    "generator.process_folder(\"camel_tex\", display_frames=False)\n",
    "#generator.process_single_mesh(\"meshes\", index=5) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/miniconda3/envs/diff3f/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "#import torch\n",
    "from diff3f import get_features_per_vertex\n",
    "from diffusion import init_pipe\n",
    "\n",
    "from utils import convert_mesh_container_to_torch_mesh, cosine_similarity, double_plot, get_colors\n",
    "from dataloaders.mesh_container import MeshContainer\n",
    "from dino import init_dino\n",
    "from sam2_setup import init_sam2\n",
    "#from functional_map import compute_surface_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_views = 50\n",
    "H = 512\n",
    "W = 512\n",
    "num_images_per_prompt = 1\n",
    "tolerance = 0.004\n",
    "random_seed = 42\n",
    "use_normal_map = True\n",
    "is_tosca = False\n",
    "\n",
    "bq = True\n",
    "use_sam = False\n",
    "use_only_diffusion = False\n",
    "use_diffusion = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(device, sam_model, dino_model, pipe, m, prompt, num_views, H, W, tolerance):\n",
    "    # Check if input is already a PyTorch3D Meshes object\n",
    "    if not hasattr(m, 'vert'):  # PyTorch3D Meshes object\n",
    "        mesh = m\n",
    "    else:  # MeshContainer object\n",
    "        mesh = convert_mesh_container_to_torch_mesh(m, device=device, is_tosca=False)\n",
    "    \n",
    "    mesh_vertices = mesh.verts_list()[0]\n",
    "\n",
    "\n",
    "    features = get_features_per_vertex(\n",
    "        device=device,\n",
    "        sam_model=sam_model,\n",
    "        pipe=pipe,\n",
    "        dino_model=dino_model,\n",
    "        mesh=mesh,\n",
    "        prompt=prompt,\n",
    "        num_views=num_views,\n",
    "        H=H,\n",
    "        W=W,\n",
    "        tolerance=tolerance,\n",
    "        use_normal_map= use_normal_map,\n",
    "        num_images_per_prompt=num_images_per_prompt,\n",
    "        mesh_vertices=mesh_vertices,\n",
    "        bq=bq,\n",
    "        use_sam = use_sam,\n",
    "        use_only_diffusion = use_only_diffusion,\n",
    "        use_diffusion = use_diffusion,\n",
    "    )\n",
    "    return features.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bfc955cf624db0bb31c8ec363e44b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pipeline_controlnet_img2img:You have disabled the safety checker for <class 'pipeline_controlnet_img2img.StableDiffusionControlNetImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "sam_model = init_sam2(device)\n",
    "pipe = init_pipe(device)\n",
    "dino_model = init_dino(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.io import load_objs_as_meshes\n",
    "\n",
    "source_file_path = \"cow_tex/cow_tex.obj\"\n",
    "target_file_path = \"camel_tex/camel_tex.obj\"\n",
    "\n",
    "source_file_path = \"meshes/cow.obj\"\n",
    "target_file_path = \"meshes/camel.obj\"\n",
    "\n",
    "source_file_path = \"SHREC20b_lores/models/cow.obj\"\n",
    "target_file_path = \"SHREC20b_lores/models/camel_a.obj\"\n",
    "\n",
    "# source_mesh = load_objs_as_meshes([source_file_path], device=device)\n",
    "# target_mesh = load_objs_as_meshes([target_file_path], device=device)\n",
    "\n",
    "source_mesh = MeshContainer().load_from_file(source_file_path)\n",
    "target_mesh = MeshContainer().load_from_file(target_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch_render with num_views=50, H=512, W=512\n",
      "Rendering completed successfully\n",
      "Starting batch_render with num_views=50, H=512, W=512\n",
      "Rendering completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:44<00:00,  4.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing features:  0\n",
      "Copied features from nearest vertices\n",
      "Time taken in mins:  8.158695892492931\n",
      "Starting batch_render with num_views=50, H=512, W=512\n",
      "Rendering completed successfully\n",
      "Starting batch_render with num_views=50, H=512, W=512\n",
      "Rendering completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:47<00:00,  4.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing features:  30\n",
      "Copied features from nearest vertices\n",
      "Time taken in mins:  8.38518809080124\n"
     ]
    }
   ],
   "source": [
    "f_source = compute_features(device, sam_model, dino_model, pipe, source_mesh, \"cow\", num_views, H, W, tolerance)\n",
    "f_target = compute_features(device, sam_model, dino_model, pipe, target_mesh, \"camel\", num_views, H, W, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2b71fc2be74ccf8bc78ca7e12e38c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c0b747332c41d38ab10696e173da39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import meshplot as mp\n",
    "\n",
    "def get_colorss(vertices):\n",
    "    \"\"\"Get colors for vertices using their normalized positions as RGB values\"\"\"\n",
    "    # If vertices is a Meshes object, get the vertices tensor and convert to numpy\n",
    "    if hasattr(vertices, 'verts_list'):\n",
    "        vertices = vertices.verts_list()[0].cpu().numpy()\n",
    "    elif torch.is_tensor(vertices):\n",
    "        vertices = vertices.cpu().numpy()\n",
    "    \n",
    "    min_coord, max_coord = np.min(vertices, axis=0, keepdims=True), np.max(vertices, axis=0, keepdims=True)\n",
    "    cmap = (vertices - min_coord)/(max_coord - min_coord)\n",
    "    return cmap\n",
    "\n",
    "def double_plot(myMesh1, myMesh2, cmap1=None, cmap2=None):\n",
    "    # Get vertices and faces from PyTorch3D Meshes if needed\n",
    "    if hasattr(myMesh1, 'verts_list'):\n",
    "        verts1 = myMesh1.verts_list()[0].cpu().numpy()\n",
    "        faces1 = myMesh1.faces_list()[0].cpu().numpy()\n",
    "    else:\n",
    "        verts1 = myMesh1.vert\n",
    "        faces1 = myMesh1.face\n",
    "        \n",
    "    if hasattr(myMesh2, 'verts_list'):\n",
    "        verts2 = myMesh2.verts_list()[0].cpu().numpy()\n",
    "        faces2 = myMesh2.faces_list()[0].cpu().numpy()\n",
    "    else:\n",
    "        verts2 = myMesh2.vert\n",
    "        faces2 = myMesh2.face\n",
    "    \n",
    "    d = mp.subplot(verts1, faces1, c=cmap1, s=[2, 2, 0])\n",
    "    mp.subplot(verts2, faces2, c=cmap2, s=[2, 2, 1], data=d)\n",
    "\n",
    "\n",
    "s = cosine_similarity(f_source.to(device),f_target.to(device))\n",
    "s = torch.argmax(s, dim=0).cpu().numpy()\n",
    "np.savetxt('s.csv', s, delimiter=',', fmt='%d')\n",
    "\n",
    "cmap_source = get_colors(source_mesh.vert); cmap_target = cmap_source[s]\n",
    "double_plot(source_mesh,target_mesh,cmap_source,cmap_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mesh sizes: Source (cow): (6935, 3), Target (camel): (9766, 3)\n",
      "Correspondence array shape: (9766,)\n",
      "Sample correspondences: [6575 6358 5907  798 4162]\n",
      "\n",
      "Evaluation Results:\n",
      "Average Error: 0.3341\n",
      "Accuracy: 0.3043\n",
      "Raw Average Error: 0.4368\n"
     ]
    }
   ],
   "source": [
    "from eval import evaluate_cow_camel\n",
    "import numpy as np\n",
    "\n",
    "# Load correspondences from CSV file\n",
    "#s = np.loadtxt('s.csv', delimiter=',', dtype=np.int64)\n",
    "\n",
    "\n",
    "# Evaluate the correspondences\n",
    "metrics = evaluate_cow_camel(\n",
    "    cow_mesh_verts=source_mesh.vert,\n",
    "    camel_mesh_verts=target_mesh.vert,\n",
    "    correspondences=s,\n",
    "    cow_gt_path='SHREC20b_lores_gts/cow.mat',\n",
    "    camel_gt_path='SHREC20b_lores_gts/camel_a.mat'\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(f\"Average Error: {metrics['average_error']:.4f}\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
